{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c99faf649cd52b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Teaching a quadruped to walk\n",
    "\n",
    "Time to try out the learning algorithms that you just implemented on a more difficult problem. The WalkerEnv implements a quadruped robot kind-of thing, see for yourself. The goal is to move in the $x$ direction as fast and as far as possible.\n",
    "\n",
    "Your goal is to implement a class `WalkerPolicy` with function `determine_actions()` just like the StochasticPolicy we used earlier to control the pendulum. Below is a template of this class, but feel free to alter it however you want. The only important thing is the `determine_actions()` function!\n",
    "\n",
    "After you implement it, copy `WalkerPolicy` into a separate file `WalkerPolicy.py` that you will upload to BRUTE together with the (optional) learned weights in a zip file. How the policy is implemented is up to you! You are constrained to only the libraries we used so far though, such as torch, numpy etc..\n",
    "\n",
    "You will get some free points just for uploading a working policy (irrelevant of the performance). Further 2 points will be awarded for successfully traversing a small distance in the x direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41290d3f9ccf033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hints\n",
    "\n",
    "There is no single easy way of doing this, but here are some suggestions on what you could try to improve your policy:\n",
    "\n",
    "1. This problem is much more difficult, than balancing a pendulum. It is a good idea to use a bit larger network than for the pendulum policy.\n",
    "\n",
    "2. You can also try to use a different optimizer, such as Adam and play with the hyperparameters.\n",
    "\n",
    "3. Using a neural network to compute the normal distribution scale $\\sigma$ can lead to too much randomness in the actions (i.e. exploration). You can use a fixed $\\sigma$ instead, or replace it with a learnable `torch.Parameter` initialized to some small constant. Make sure, you run it through an exponential, or softplus function to ensure $\\sigma$ is positive.\n",
    "\n",
    "4. The exploration can also be reduced by penalizing the variance of the action distribution in an additional loss term.\n",
    "\n",
    "5. If you see some undesirable behaviour, you can tweak the reward function to penalize it. Even though the $x$ distance is all we care about, adding extra terms to the reward can help guide the learning process (This is known as reward shaping). Simply define a reward function mapping the state $s_{t+1}$ and action $a_t$ to a scalar reward $r_t$ and put it in the config dictionary under the key `'reward_fcn'`. See the `WalkerEnv` class for the implementation of the default reward.\n",
    "\n",
    "6. Using the normal distribution on a bounded action space can lead to certain problems caused by action clipping. This can be mitigated by using a different distribution, such as the Beta distribution. See the `torch.distributions.beta` module for more information. (Note that Beta distribution is defined on the interval [0,1] and works better with parameters $\\alpha,\\beta \\geq 1$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot run with the visualization, you can set this to False\n",
    "VISUALIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d52d6512e1dc81e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:30:41.415964800Z",
     "start_time": "2023-11-30T16:30:40.816557700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "577cd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walker_reward(state, action):\n",
    "#     \"\"\"reward function for the walker environment, state is [29] vector, action is [8] vector\"\"\"\n",
    "#     pos = state[:15]  # first 15 elements of state vector are generalized coordinates [xyz, quat, joint_angles]\n",
    "#     vel = state[15:]  # last 14 elements of state vector are generalized velocities [xyz_vel, omega, joint_velocities]\n",
    "#     return vel[0]  # return the x velocity as the reward by default\n",
    "# def walker_reward(state, action):\n",
    "#     # Reward = x velocity\n",
    "#     vel = state[0, 15:]  # velocity is last part of state\n",
    "#     return vel[0]  \n",
    "def walker_reward(state, action):\n",
    "    pos = state[:, :15]  # first 15 elements => generalized coordinates\n",
    "    vel = state[:, 15:]  # last 14 elements => generalized velocities\n",
    "    return vel[:, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e64a34ffdb26d39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:46.270913500Z",
     "start_time": "2023-11-30T09:51:46.157914600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    }
   ],
   "source": [
    "# This is the configuration for the Walker environment\n",
    "# N is the number of robots controlled in parallel\n",
    "# vis is a boolean flag to enable visualization\n",
    "# !! IMPORTANT track is a boolean flag to enable camera tracking of a particular robot, this is useful when evaluating the performance of the policy after training\n",
    "# reward_fcn is the reward function that the environment will use to calculate the reward\n",
    "config = {'N': 1, 'vis': VISUALIZE, \"track\": 0, \"reward_fcn\": walker_reward}\n",
    "env = WalkerEnv(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1159688c1501d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:53:18.623643200Z",
     "start_time": "2023-11-30T09:53:18.600695400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n",
      "Episode 1/100, Return=-2.038\n",
      "Episode 2/100, Return=-1.899\n",
      "Episode 3/100, Return=1.107\n",
      "Episode 4/100, Return=-1.725\n",
      "Episode 5/100, Return=1.546\n",
      "Episode 6/100, Return=-2.912\n",
      "Episode 7/100, Return=-1.873\n",
      "Episode 8/100, Return=-2.408\n",
      "Episode 9/100, Return=-2.331\n",
      "Episode 10/100, Return=-0.821\n",
      "Episode 11/100, Return=-3.984\n",
      "Episode 12/100, Return=0.689\n",
      "Episode 13/100, Return=1.126\n",
      "Episode 14/100, Return=2.691\n",
      "Episode 15/100, Return=-2.599\n",
      "Episode 16/100, Return=-3.099\n",
      "Episode 17/100, Return=-0.099\n",
      "Episode 18/100, Return=0.587\n",
      "Episode 19/100, Return=1.409\n",
      "Episode 20/100, Return=-1.312\n",
      "Episode 21/100, Return=-4.106\n",
      "Episode 22/100, Return=4.475\n",
      "Episode 23/100, Return=4.244\n",
      "Episode 24/100, Return=0.888\n",
      "Episode 25/100, Return=-1.132\n",
      "Episode 26/100, Return=-0.909\n",
      "Episode 27/100, Return=-2.858\n",
      "Episode 28/100, Return=0.402\n",
      "Episode 29/100, Return=-2.178\n",
      "Episode 30/100, Return=0.184\n",
      "Episode 31/100, Return=2.474\n",
      "Episode 32/100, Return=-0.562\n",
      "Episode 33/100, Return=0.241\n",
      "Episode 34/100, Return=-1.053\n",
      "Episode 35/100, Return=-2.457\n",
      "Episode 36/100, Return=-0.843\n",
      "Episode 37/100, Return=-1.605\n",
      "Episode 38/100, Return=-1.151\n",
      "Episode 39/100, Return=-3.099\n",
      "Episode 40/100, Return=-1.083\n",
      "Episode 41/100, Return=-0.316\n",
      "Episode 42/100, Return=-0.425\n",
      "Episode 43/100, Return=-1.853\n",
      "Episode 44/100, Return=-1.033\n",
      "Episode 45/100, Return=-1.324\n",
      "Episode 46/100, Return=-1.380\n",
      "Episode 47/100, Return=-2.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\rl-homework-venv\\lib\\site-packages\\glfw\\__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m a_np \u001b[38;5;241m=\u001b[39m a_torch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m s_next, r \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Store experience\u001b[39;00m\n\u001b[0;32m     62\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(s_torch)\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\environment\\WalkerEnv.py:94\u001b[0m, in \u001b[0;36mWalkerEnv.vector_step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     91\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(rew)  \u001b[38;5;66;03m# reward is the negative sin of the angle\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# if rendering is enabled, render after each simulation step\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_actions \u001b[38;5;241m=\u001b[39m ctrl\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_walkers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(states), np\u001b[38;5;241m.\u001b[39marray(rewards)\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\environment\\mujoco_env_custom.py:125\u001b[0m, in \u001b[0;36mextendedEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m extended_Viewer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_setup()\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth_array\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\rl-homework-venv\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:437\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 437\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\rl-homework-venv\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:377\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[1;34m()\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m():\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# fill overlay items\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m     render_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\rl-homework-venv\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:607\u001b[0m, in \u001b[0;36mWindowViewer._create_overlay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_overlay(\n\u001b[0;32m    602\u001b[0m             topleft, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdvance simulation by one step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[right arrow]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    603\u001b[0m         )\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_overlay(\n\u001b[0;32m    605\u001b[0m     topleft, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReferenc[e] frames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvopt\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOff\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[H]ide Menu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    609\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_path \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bogda\\Deep Reinforcement Learning\\hw5-rl-main\\rl-homework-venv\\lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:77\u001b[0m, in \u001b[0;36mBaseRender.add_overlay\u001b[1;34m(self, gridpos, text1, text2)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_context_current\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_overlay\u001b[39m(\u001b[38;5;28mself\u001b[39m, gridpos: \u001b[38;5;28mint\u001b[39m, text1: \u001b[38;5;28mstr\u001b[39m, text2: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Overlays text on the scene.\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gridpos \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlays:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy\n",
    "\n",
    "###############################################################################\n",
    "# Simple reward function for the Walker\n",
    "###############################################################################\n",
    "def walker_reward(state, action):\n",
    "    # Here, 'state' is shape (29,) => 1D array\n",
    "    pos = state[:15]  # first 15 elements => generalized coordinates\n",
    "    vel = state[15:]  # last 14 elements => generalized velocities\n",
    "    return vel[0]     # x velocity\n",
    "\n",
    "###############################################################################\n",
    "# Environment configuration\n",
    "###############################################################################\n",
    "config = {\n",
    "    'N': 1,\n",
    "    'vis': VISUALIZE,   # set True if you want to see the robot\n",
    "    'track': 0,\n",
    "    'reward_fcn': walker_reward\n",
    "}\n",
    "\n",
    "env = WalkerEnv(config)\n",
    "\n",
    "###############################################################################\n",
    "# Policy and optimizer\n",
    "###############################################################################\n",
    "policy = WalkerPolicy(state_dim=29, action_dim=8)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "\n",
    "###############################################################################\n",
    "# Training hyperparameters\n",
    "###############################################################################\n",
    "num_episodes = 100\n",
    "max_episode_steps = 200\n",
    "gamma = 0.99\n",
    "\n",
    "###############################################################################\n",
    "# Training loop\n",
    "###############################################################################\n",
    "for episode in range(num_episodes):\n",
    "    s = env.vector_reset()  # shape (1,29), but we'll treat it as array\n",
    "    states, actions, rewards, logps = [], [], [], []\n",
    "\n",
    "    for t in range(max_episode_steps):\n",
    "    # Convert state to torch\n",
    "        s_torch = torch.tensor(s, dtype=torch.float32, requires_grad=False)  # shape (1, 29)\n",
    "\n",
    "        # Forward pass through policy\n",
    "        a_torch, log_prob = policy.sample_actions_and_log_prob(s_torch)  # Modify your policy to return both action and log-prob\n",
    "\n",
    "        # Convert to numpy to step the environment\n",
    "        a_np = a_torch.detach().cpu().numpy()\n",
    "\n",
    "        # Step environment\n",
    "        s_next, r = env.vector_step(a_np)\n",
    "\n",
    "        # Store experience\n",
    "        states.append(s_torch)\n",
    "        actions.append(a_torch)\n",
    "        rewards.append(torch.tensor(r, dtype=torch.float32))\n",
    "        logps.append(log_prob)  # Append log-probabilities\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "    # Discounted returns\n",
    "    rews = torch.cat(rewards, dim=0)  # shape (T,1)\n",
    "    T = rews.shape[0]\n",
    "    returns = torch.zeros_like(rews)\n",
    "    running_sum = 0.0\n",
    "\n",
    "    for i in reversed(range(T)):\n",
    "        running_sum = rews[i] + gamma * running_sum\n",
    "        returns[i] = running_sum\n",
    "\n",
    "    # Convert everything into a single batch\n",
    "    all_states = torch.cat(states, dim=0)    # (T, 29)\n",
    "    all_actions = torch.cat(actions, dim=0)  # (T, 8)\n",
    "    all_logps = torch.cat(logps, dim=0)      # (T, 1)\n",
    "    all_returns = returns                    # (T, 1)\n",
    "\n",
    "    # Policy gradient loss (placeholder: logp=0 => no gradient)\n",
    "    pg_loss = - (all_logps * all_returns).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pg_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Simple printout of returns\n",
    "    ep_return = all_returns[0].item()\n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Return={ep_return:.3f}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Done training!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-homework-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
